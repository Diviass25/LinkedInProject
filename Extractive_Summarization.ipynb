{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ce6cec"
      },
      "source": [
        "## Project Requirements\n",
        "Perform extractive summarization on a given text, involving steps like text preprocessing, sentence scoring, and extraction of summary sentences, then display the final summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f03bb56"
      },
      "source": [
        "## Load or Generate Text Data\n",
        "\n",
        "Load an existing text document or generate a sample text for the summarization project. This will be the input document from which we will extract the summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89b10322",
        "outputId": "e46f6ccb-422e-4a3b-cd48-a60299797be0"
      },
      "source": [
        "text_data = \"\"\"Artificial intelligence (AI) is rapidly transforming various aspects of our lives, from how we work to how we interact with technology. Machine learning, a subset of AI, enables systems to learn from data without explicit programming, leading to advancements in fields like natural language processing and computer vision. Deep learning, a further specialization within machine learning, utilizes neural networks with multiple layers to uncover intricate patterns in vast datasets. These technologies are at the forefront of innovation, powering applications such as self-driving cars, medical diagnostics, and personalized recommendation systems. Ethical considerations and responsible development are crucial as AI continues to evolve and become more integrated into society. The potential benefits are immense, but so are the challenges, requiring careful thought and collaboration from researchers, policymakers, and the public. Understanding the fundamentals of AI is becoming increasingly important for everyone in the modern world. This text provides a brief overview of the key concepts and impact of artificial intelligence.\"\"\"\n",
        "\n",
        "print(\"Sample text data loaded successfully.\")\n",
        "print(f\"Length of text_data: {len(text_data)} characters\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text data loaded successfully.\n",
            "Length of text_data: 1133 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eb84175"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "Clean and preprocess the text data. This typically involves sentence tokenization, word tokenization, removing stop words, and potentially stemming or lemmatization, to prepare it for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1b2ab04",
        "outputId": "73069b07-0289-4297-bbc1-5b7f88abd4ca"
      },
      "source": [
        "try:\n",
        "    import nltk\n",
        "except ImportError:\n",
        "    print(\"NLTK not found. Installing NLTK...\")\n",
        "    !pip install nltk\n",
        "    import nltk\n",
        "    print(\"NLTK installed successfully.\")\n",
        "else:\n",
        "    print(\"NLTK is already installed.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4b4f1b6"
      },
      "source": [
        "To preprocess the text data as instructed, I will import necessary NLTK modules, download required NLTK data, tokenize the text into sentences, and then iterate through each sentence to perform word tokenization, lowercasing, punctuation removal, stop word filtering, and lemmatization, storing the results in `preprocessed_sentences`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f654f507",
        "outputId": "4aadbd7f-a74f-405a-e0c5-30d2f278907d"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "print(\"Downloading NLTK data...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True) # Open Multilingual Wordnet for lemmatizer\n",
        "nltk.download('punkt_tab', quiet=True) # Added to fix LookupError\n",
        "print(\"NLTK data downloaded.\")\n",
        "\n",
        "# 3. Tokenize the text_data into individual sentences\n",
        "sentences = sent_tokenize(text_data)\n",
        "\n",
        "# 4. Initialize a WordNetLemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 5. Create an empty list, preprocessed_sentences\n",
        "preprocessed_sentences = []\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 6. Iterate through each sentence for preprocessing\n",
        "for sentence in sentences:\n",
        "    # a. Tokenize the sentence into words\n",
        "    words = word_tokenize(sentence)\n",
        "\n",
        "    preprocessed_words = []\n",
        "    for word in words:\n",
        "        # b. Convert each word to lowercase\n",
        "        word = word.lower()\n",
        "\n",
        "        # c. Remove punctuation from the words\n",
        "        # Check if the word is purely punctuation\n",
        "        if word in string.punctuation:\n",
        "            continue\n",
        "\n",
        "        # Remove punctuation from within words (e.g., 'word.' -> 'word')\n",
        "        word = word.strip(string.punctuation)\n",
        "\n",
        "        # Skip if the word becomes empty after punctuation removal\n",
        "        if not word:\n",
        "            continue\n",
        "\n",
        "        # d. Filter out stopwords\n",
        "        if word not in stop_words:\n",
        "            # e. Apply lemmatization to each remaining word\n",
        "            # Lemmatize as a noun by default, can add pos tag if more precision needed\n",
        "            lemmatized_word = lemmatizer.lemmatize(word)\n",
        "            preprocessed_words.append(lemmatized_word)\n",
        "\n",
        "    # f. Join the lemmatized words back into a string and append it to preprocessed_sentences\n",
        "    preprocessed_sentences.append(' '.join(preprocessed_words))\n",
        "\n",
        "# 7. Print the first few preprocessed sentences to verify\n",
        "print(\"\\nFirst 3 preprocessed sentences:\")\n",
        "for i, sent in enumerate(preprocessed_sentences[:3]):\n",
        "    print(f\"Sentence {i+1}: {sent}\")\n",
        "\n",
        "print(f\"\\nTotal preprocessed sentences: {len(preprocessed_sentences)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK data...\n",
            "NLTK data downloaded.\n",
            "\n",
            "First 3 preprocessed sentences:\n",
            "Sentence 1: artificial intelligence ai rapidly transforming various aspect life work interact technology\n",
            "Sentence 2: machine learning subset ai enables system learn data without explicit programming leading advancement field like natural language processing computer vision\n",
            "Sentence 3: deep learning specialization within machine learning utilizes neural network multiple layer uncover intricate pattern vast datasets\n",
            "\n",
            "Total preprocessed sentences: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64ce1eae"
      },
      "source": [
        "## Implement Sentence Scoring\n",
        "\n",
        "Develop a method to score the importance of each sentence in the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccc23e8d"
      },
      "source": [
        "To implement sentence scoring based on word frequency, I will first calculate the frequency of each word across all preprocessed sentences, then iterate through each sentence to sum the frequencies of its words, storing these sums as sentence scores, and finally print the scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f54d9b8b",
        "outputId": "3821a842-6ebb-4f52-f5ad-05118bbad47d"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "# 1. Calculate the frequency of each word across all preprocessed_sentences.\n",
        "word_frequencies = defaultdict(int)\n",
        "for sentence in preprocessed_sentences:\n",
        "    for word in sentence.split():\n",
        "        word_frequencies[word] += 1\n",
        "\n",
        "print(\"Top 10 word frequencies:\")\n",
        "# Sort word_frequencies by value in descending order and print the top 10\n",
        "sorted_word_frequencies = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "for word, freq in sorted_word_frequencies[:10]:\n",
        "    print(f\"  {word}: {freq}\")\n",
        "\n",
        "# 2. Create an empty list called sentence_scores\n",
        "sentence_scores = []\n",
        "\n",
        "# 3. Iterate through each preprocessed_sentence\n",
        "for sentence in preprocessed_sentences:\n",
        "    # a. Initialize a current_sentence_score to 0.\n",
        "    current_sentence_score = 0\n",
        "    # b. For each word in the current preprocessed_sentence (split by spaces):\n",
        "    for word in sentence.split():\n",
        "        # i. Add the word's frequency to current_sentence_score.\n",
        "        current_sentence_score += word_frequencies[word]\n",
        "    # c. Append current_sentence_score to the sentence_scores list.\n",
        "    sentence_scores.append(current_sentence_score)\n",
        "\n",
        "# 4. Print the sentence_scores list to inspect the calculated scores.\n",
        "print(\"\\nCalculated sentence scores:\")\n",
        "for i, score in enumerate(sentence_scores):\n",
        "    print(f\"Sentence {i+1} score: {score}\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 word frequencies:\n",
            "  ai: 4\n",
            "  learning: 3\n",
            "  artificial: 2\n",
            "  intelligence: 2\n",
            "  technology: 2\n",
            "  machine: 2\n",
            "  system: 2\n",
            "  rapidly: 1\n",
            "  transforming: 1\n",
            "  various: 1\n",
            "\n",
            "Calculated sentence scores:\n",
            "Sentence 1 score: 17\n",
            "Sentence 2 score: 27\n",
            "Sentence 3 score: 21\n",
            "Sentence 4 score: 14\n",
            "Sentence 5 score: 14\n",
            "Sentence 6 score: 11\n",
            "Sentence 7 score: 12\n",
            "Sentence 8 score: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3a3385"
      },
      "source": [
        "## Extract Summary Sentences\n",
        "\n",
        "Select the top N highest-scoring sentences from the original document to form the extractive summary. The value of N can be adjusted based on the desired summary length.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d43a4a61",
        "outputId": "e1979615-b4e1-4e4b-9e91-4b406f7e41af"
      },
      "source": [
        "n = 3 # Desired number of summary sentences\n",
        "\n",
        "# Ensure n does not exceed the total number of sentences\n",
        "if n > len(sentences):\n",
        "    n = len(sentences)\n",
        "\n",
        "# Create a list of (score, original_sentence) tuples\n",
        "sentence_score_pairs = []\n",
        "for i, score in enumerate(sentence_scores):\n",
        "    sentence_score_pairs.append((score, sentences[i]))\n",
        "\n",
        "# Sort the list of tuples by score in descending order\n",
        "sentence_score_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# Select the top 'n' sentences to form the summary\n",
        "summary_sentences = []\n",
        "for i in range(n):\n",
        "    summary_sentences.append(sentence_score_pairs[i][1]) # Append the original sentence\n",
        "\n",
        "print(f\"Extracted Summary (Top {n} sentences):\")\n",
        "for i, sent in enumerate(summary_sentences):\n",
        "    print(f\"Summary Sentence {i+1}: {sent}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Summary (Top 3 sentences):\n",
            "Summary Sentence 1: Machine learning, a subset of AI, enables systems to learn from data without explicit programming, leading to advancements in fields like natural language processing and computer vision.\n",
            "Summary Sentence 2: Deep learning, a further specialization within machine learning, utilizes neural networks with multiple layers to uncover intricate patterns in vast datasets.\n",
            "Summary Sentence 3: Artificial intelligence (AI) is rapidly transforming various aspects of our lives, from how we work to how we interact with technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4da124e1"
      },
      "source": [
        "## Display Extracted Summary\n",
        "\n",
        "Present the generated extractive summary to clearly show the result of the summarization process.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "097bc78b",
        "outputId": "0bc2003b-a22a-4413-eb3e-19a2da5b283b"
      },
      "source": [
        "print(\"\\nFinal Extracted Summary:\")\n",
        "for i, sent in enumerate(summary_sentences):\n",
        "    print(f\"{i+1}. {sent}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Extracted Summary:\n",
            "1. Machine learning, a subset of AI, enables systems to learn from data without explicit programming, leading to advancements in fields like natural language processing and computer vision.\n",
            "2. Deep learning, a further specialization within machine learning, utilizes neural networks with multiple layers to uncover intricate patterns in vast datasets.\n",
            "3. Artificial intelligence (AI) is rapidly transforming various aspects of our lives, from how we work to how we interact with technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e53342a7"
      },
      "source": [
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Text Data Initialization**: A sample text document of 1133 characters related to Artificial Intelligence was successfully loaded as the input for summarization.\n",
        "*   **Comprehensive Text Preprocessing**: The text underwent several preprocessing steps using NLTK, including sentence tokenization (resulting in 8 sentences), word tokenization, conversion to lowercase, removal of punctuation, filtering of common English stopwords, and lemmatization. This process required downloading necessary NLTK data like 'punkt', 'stopwords', 'wordnet', 'omw-1.4', and 'punkt_tab'.\n",
        "*   **Word Frequency Analysis**: Word frequencies were calculated across all preprocessed sentences. For example, 'ai' appeared 4 times, and 'learning' appeared 3 times, indicating their relative importance.\n",
        "*   **Sentence Scoring Implementation**: Each preprocessed sentence was assigned a score by summing the frequencies of its constituent words. Scores ranged from 11 to 27 for the sentences, with \"Machine learning, a subset of AI, enables systems to learn from data without explicit programming, leading to advancements in fields like natural language processing and computer vision\" receiving the highest score of 27 (from its preprocessed form).\n",
        "*   **Extractive Summary Generation**: The top 3 highest-scoring sentences from the original text were successfully extracted to form the summary. These included:\n",
        "    1.  \"Machine learning, a subset of AI, enables systems to learn from data without explicit programming, leading to advancements in fields like natural language processing and computer vision.\"\n",
        "    2.  \"Deep learning, a further specialization within machine learning, utilizes neural networks with multiple layers to uncover intricate patterns in vast datasets.\"\n",
        "    3.  \"Artificial intelligence (AI) is rapidly transforming various aspects of our lives, from how we work to how we interact with technology.\"\n",
        "*   **Final Summary Presentation**: The generated extractive summary was clearly displayed, listing the three selected sentences in an ordered format."
      ]
    }
  ]
}